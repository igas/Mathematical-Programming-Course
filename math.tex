\documentclass[12pt,a5paper]{scrbook}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usepgflibrary{shapes.misc}
\usepackage[a5paper]{geometry}
\geometry{left=2cm}
\geometry{right=1cm}
%\geometry{top=2cm}
%\geometry{bottom=2cm}

% newline after \paragraph
\makeatletter 
\renewcommand\paragraph{\@startsection{paragraph}{4}{0mm}% 
{-\baselineskip} % 
{0.5\baselineskip} % 
{\normalfont\bfseries}}% 
\makeatother 

\begin{document}
  \pagestyle{plain}
  \begin{titlepage}
    \newgeometry{left=1cm,right=1cm,top=3cm,bottom=1cm}
    \begin{center}
      \Huge{Ю.~Я.~Зубарев}
    \end{center}
    \vfill
    \vfill
    \begin{center}
      \Large{\textbf{Математическое программирование\\
      Конспект лекций}}
    \end{center}
    \vfill
    \vfill
    \vfill
    \restoregeometry
  \end{titlepage}
  \tableofcontents
  \newpage
  Математическое программирование - это методы принятия оптимальных решений.\par
  История развития методов математического программирования:
  \begin{enumerate}
    \item Системы автоматизации управления.\\Появились работы по оптимальным автоматическим системам.
    \item Методы, связанные с ракетной техникой.\\Решалась задача об оптимальной траектории ракет.
    \item Решение военных, экономических и административных задач.
    \item Логистика.
  \end{enumerate}\par
  Существовала и классическая теория оптимизации, но она не получила распространения, т.к. не учитывает ограничений.\par
  Критерий оптимальности - это формализованное правило, позволяющее производить сравнительную оценку различных вариантов (решений, процедур, операций...) и выбор наилучшей из них.\par
  Любая система (процесс, операция) характеризуется какими либо операциями.\par
\{СХЕМА!\}
  \begin{enumerate}
    \item Астрономические наблюдения.\\Судно - материальная точка.
    \item Волнения.\\Судно - линейная стохастическая модель.
    \item Разворот судна в порту.\\Нелинейная модель.
  \end{enumerate}\par
  Цель: свести решение задачи к типовым программным продуктам.\par
  Математическая формулировка задачи\par
  \(
  k(q_1, q_2\ldots q_n)\to min (max)
  \)
  \par
  3 показателя ограничений:\par
  \(
  k_{\rho}(q_1, q_2\ldots q_n)\geq k_{\rho_{min}}
  \)\par
  \(
  k_{\rho}(q_1, q_2\ldots q_n)\leq k_{\rho_{max}}
  \)\par
  \(
  k_{\rho}(q_1, q_2\ldots q_n) = k_{\rho_{0}}
  \)\par
  $\rho = 1, 2, \ldots m$ (обычно затраты) применяется редко\par
  Рассмотрим 1й случай:\par
  $k$ - линейные функции от параметров $q$. В этом случае мы рассматриваем задачи линейного программирования.\par
  $k$ - нелинейные функции. Тогда мы рассматриваем задачи нелинейного программирования. Размерность задачи нелинейного программирования 2\,--\,5 параметров.\par
  $q$ - целочисленные, тогда имеем дело с целочисленным программированием (линейным и нелинейным). Разбиение задачи на подзадачи.\par
  Стохастическое программирование - нам не известны точные значения $q$, но известны их вероятностные характеристики или интервалы изменения их значений.\par
  \chapter{Методы одномерной оптимизации}
  \pagestyle{headings}
  $$k(q)\to min(max)$$
  $$q_{min}\leq q \leq q_{max}$$
  Унимодальная функция\\
  \begin{tikzpicture}
    \draw[->] (-0.5,0)--(6,0) node[anchor=north] {};
    \draw[->] (0,-0.5)--(0,3.5) node[anchor=east] {};
    \draw (1,0) node[below]{$q_{min}$} .. controls (1.5,4) and (4.5,4) .. (5,0) node[below]{$q_{max}$};
    \draw[dashed] (3,0) node[below]{$q_0$} -- (3,3);
  \end{tikzpicture}
  \section{Методы с последовательным уменьшением интервала неопределенности}
  $\Delta_0 = q_{max} - q_{min}$ -- это интервал неопределенности.\par
  Функция униполярная $\Rightarrow$ мы можем уменьшить интервал неопределенности.\par
  \{ГРАФИКИ\}\par
  Совершая различные вычисления, мы уменьшаем интервал неопределенности.
  \newpage
  \subsection{Метод дихотомии (деление пополам)}
  TODO: ГРАФИК
  \par
  Делим до тех пор, пока отрезок не будет достаточно мал.
  \par
  На каждой итерации 2 вычисления.
  \par
  $$\Delta_2 = \frac{\Delta_0}{2} + \frac{\varepsilon}{2}$$
  $$\Delta_4 = \frac{\Delta_0}{2^2} + \bigg(1 - \frac{1}{2^2}\bigg)\varepsilon$$
  $$\Delta_r = \frac{\Delta_0}{2^{r/2}} + \bigg(\underbrace{1 - \frac{1}{2^{r/2}}}_{\rightarrow 1}\bigg)\varepsilon$$
  \par
  Если считать, что $\varepsilon$ можно пренебречь.
  $$\Delta_r = \frac{\Delta_0}{2^{r/2}}$$
  \par
  Преимущество: Простота
  \par
  Недостаток: Плохая сходимость
  \subsection{Золотое сечение}
  Основан на следующий допущениях:
  \par
  $$\Delta_{r-1}, \Delta_r, \Delta_{r+1}$$
  \par
  \begin{enumerate}
    \item $\frac{\Delta_{r-1}}{\Delta_r} = \frac{\Delta_r}{\Delta_{r+1}} = \tau$
    \item $\Delta_{r-1} = \Delta_r + \Delta_{r+1}$
    TODO: заметка на поле
  \end{enumerate}
  $$\frac{\Delta_{r-1}}{\Delta_{r+1}} = \frac{\Delta_r}{\Delta_{r+1}} + 1$$
  $$\tau^\varrho = \tau + 1$$
  $$\tau = 1,618$$
  \par
  TODO: график
  \par
  На каждой итерации только одно вычисление, а на следующем берем результат предыдущего. На первой итерации 2 вычисления.
  $$\frac{\Delta_0}{\Delta_r} = \tau^{r-1}$$
  \par
  Преимущества: хорошая сходимость.
  \par
  Недостатки: достаточная сложность алгоритма.
  \section{Шаговые методы}
  При каждом последующем шаге результат должен быть не хуже предыдущего.
  \par
  \subsection{Метод с постоянным шагом}
  TODO: Графики
  $$k(q_2)>k(q_1)$$ TODO: график
  \subsection{Метод с переменным (пропорциональным) шагом}
  TODO: График
  $$q_3 = q_2 + \theta\frac{k(q_2) - k(q_1)}{\underbrace{q_2 - q_1}_{\lambda}}$$
  $$q^{(r)} = q^{(r-1)} + \theta = \frac{k(q^{r-1}) - k(q^{r+1})}{q^{(r+1)} - q^{(r-1)}}$$
  $$\theta = \frac{k(q^{(2)}) - k(q^{(1)})}{\lambda}$$
  $$\theta = \frac{\lambda^2}{k(q^{(2)}) - k(q^{(1)})}$$
  \par
  Частный случай градиентного метода. Шаг завичит от вида поверхности.
  \subsection{Метод с переменным шагом с последующей квадратичной апроксимацией}
  $$k = TODO: дописать функцию$$
  TODO: график
  \par
  Увеличиваем шаг в 2 раза, TODO: дописать параграф
  $$q^{r+1} = q^r + \lambda \cdot 2^{r-2}$$
  $$\lambda_1 = 2^{r-3}$$
  \par
  Отбрасываем одну из крайних точек, которая наихудшая.
  $$q_{max(min)} = q_0 + \frac{\lambda_1}{2} \cdot \frac{k(q_-) - k(q_+)}{k(q_-) - 2k(q_0) + k(q_+)}$$
  \par
  Подставим наши значения:
  $$q_{max} = 8 + \frac{4}{2} \cdot \frac{91 - 75}{91 - 2 \cdot 99 + 75} = 8 + 2 \cdot \frac{+16}{-32} = 7$$
  \par
  Порядок действий:
  \begin{enumerate}
    \item С каждым шагом величина шага увеличивается вдвое.
    \item Движение продолжается до тех пор пока значение $k$ не начнет уменьшаться.
    \item Берется половина последнего интервала и определяется значение показателя в дополнительной точке. Из рассматриваемых 4-х точек отбрасывается точка с наименьшим(наибольшим) значением.
    \item По формуле определяется $max(min)$ квадратичной зависимости, которая проходит через 3 оставшиеся точки.
    \item В случае необходимости уменьшают на порядок интервал и заново начинаем процедуру с полученной экстремальной точки.
  \end{enumerate}
  \chapter{Основы нелинейного программирования}
  Множество - это совокупность раздельных объектов, рассматриваемых в данной задаче как единое целое.
  \par
  Конечное множество имеет конечное количество элементов меньше натурального числа $N$.
  \par
  Если множество упорядочено - это картеж.
  \par
  $$
    q^{(1)} =
    \begin{bmatrix}
      q_{11}\\
      q_{12}\\
      \vdots
    \end{bmatrix}
  $$
  $$\bar{q}^{\,1T}$$
  \par
  Между точками можно совершать операции и выделять соотношения.
  \begin{enumerate}
    \item Расстояния\\$\bar{q}^{\,(1)}, \bar{q}^{\,(2)}, \bar{q}^{\,(3)}$
  \end{enumerate}
  \begin{enumerate}
    \item Аксиома идентичности\\$d(\bar{q}^{\,(1)}, \bar{q}^{\,(2)}) = 0$
    \item Аксиома симметрии\\$d(\bar{q}^{\,(1)}, \bar{q}^{\,(2)}) = (\bar{q}^{\,(2)}, \bar{q}^{\,(1)})$
    \item Аксиома треугольника\\$d(\bar{q}^{\,(1)}, \bar{q}^{\,(3)})\leq d(\bar{q}^{\,(1)}, \bar{q}^{\,(2)}) + d(\bar{q}^{\,(2)}, \bar{q}^{\,(3)})$
  \end{enumerate}
  \par
  Если на множестве определены операции, то это множество обладает структурой (называют пространством)
  \par
  Если в пространство определяло расстояние (метрика), то пространство называют TODO.
  \par
  В судовых сетях прокладка кабеля осуществляется по Манхеттеновскому расстоянию.
  \par
  Евклидово расстояние:
  $$
    d
      \Big(
        \bar{q}^{\,\left(1\right)}, \bar{q}^{\,\left(2\right)}
      \Big)
      =
      \left[
        \sum_{i = 1}^{n}
        \left[
          q^{(1)}_{i} - q^{(2)}
        \right]^2
      \right]^{\frac{1}{2}}
  $$
  \par
  Общая формула расстояния:
  $$
  	d
      \Big(
        \bar{q}^{\,\left(1\right)}, \bar{q}^{\,\left(2\right)}
      \Big)
      =
      \left[
        \sum_{i = 1}^{n}
        \left[
          q^{(1)}_{i} - q^{(2)}
        \right]^p
      \right]^{\frac{1}{p}},\; p \to \infty
  $$
  $$
  	d
      \Big(
        \bar{q}^{\,\left(1\right)}, \bar{q}^{\,\left(2\right)}
      \Big)
      = max
      \bigg[
        \Big(
          q_1^{(1)} - q_1^{(2)}
        \Big)
        \Big(
          q_2^{(1)} - q_2^{(2)}
        \Big)
        \ldots
        \Big(
          q_n^{(1)} - q_n^{(2)}
        \Big)
      \bigg]
  $$
  \par
  Бесконечное ограниченное множество
  \par
  $d\big(\bar{q}^{(1)}, \bar{q}^{(2)}\big) < M$, то множество бесконечно, но ограничено.
  \par
  Сфера:
  \par
  $d\big(\bar{q}, \bar{a}\big) = R$
  \par
  Шар:
  \par
  Замкнутый: $d\big(\bar{q}, \bar{a}\big) \leq R$
  \par
  Открытый: $d\big(\bar{q}, \bar{a}\big) < R$
  \par
  TODO: пятно
  \par
  Пусть $q_1, q_2\hspace{20pt} \lambda\bar{q}^{(1)} + (1 - \lambda)\bar{q}^{2(1)}$, то область называется выпуклой.
  \par
  Область называется замкнутой, если все граничные области принадлежат ей.
  \par
  Теорема Веерштрасса
  \par
  Если функция определена в замкнутом ограниченным и непустом множестве, то функция в этом множестве хотя бы один раз принимает $max$ и $min$ значения.
  \par
  Непустое множество - это множество содержащее хотябы одну точку.
  \par
  Экстремумы
  \par
  Где ищем экстремумы:
  TODO: какнибудь объеденить 1й и 2й пункт
  \begin{enumerate}
  	\item Стационарная точка.
  	\item Точка, где нет производной.
  	\item Граничные точки (условный экстремум).
  \end{enumerate}
  Классический метод оптимизации
  \par
  $$k(q_1, q_2, \ldots, q_r)$$
  \begin{enumerate}
  	\item Считается что функция гладкая и имеет в каждой точке 1ю и 2ю производные.
  	\item Не учитываем ограничения ни на $q$, ни на $k$.
  \end{enumerate}
  $$q^{0T} = \Big[q_1^0, q_2^0, \ldots, q_n^0\Big]$$
  $$k(\bar{q}) = k(\bar{q}^0_1) + \sum_{i = 1}^n\Bigg(\frac{\partial k}{\partial q_1}\Bigg)_0\Delta q_1 + \frac{1}{2}\sum^n_{i, j = 1}\Bigg(\frac{\partial^2 k}{\partial q_i\partial q_j}\Bigg)_0\Delta q_i\Delta q_j$$
  $$\Delta q_1 = q_{i_0} - q_1$$
  $$\Delta q_1 = q_{i_0} - q_j$$
  \par
  Если точка стационарная, то $\sum\Big(\frac{\partial k}{\partial q_i}\Big)_0 = 0$, тогда\\$k(q^0) - k(\bar{q}_1) = - \frac{1}{2}\sum_{i, j = 1}^n\Big(\frac{\partial^2k}{\partial q_i\partial q_j}\Big)_0 \Delta q_i\Delta q_j$
  \par
  Максимуму соответствует отрицательная квадратичная форма:
  $$\frac{\partial^2k}{\partial q_i q_j} = a_{ij}$$
  \par
  Введем матрицу Гессе:
  $$
  	\mathcal{J}(\bar{q}) =
  	\begin{bmatrix}
  	  a_{11} & a_{12} & a_{13} & \cdots & a_{1n}\\
  	  a_{21} & a_{22} & a_{23} & \cdots & a_{2n}\\
  	  \vdots & \vdots & \vdots & \ddots & \vdots\\
  	  a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}\\
  	\end{bmatrix}
  $$
  \par
  Квадратическая форма
  $$\bar{q}^T\mathcal{J}(\bar{q})\bar{q}$$
  \par
  Необходимо определить свойства этой квадратичной формы. Рассмотрим определитель матрицы Гессе - гессиан.
  \par
  Критерий Сильвестра
  \par
  Если все ограничения положительны, то мы имеем положительную квадратичную форму, значит $min$.
  \par
  Если определители меняют знак (знакопеременные) то, тогда квадратичная форма отрицательная, следовательно мы имеем дело с $max$.
  \par
  Если не соблюдается ни первое, ни второе условия, такая точка - седло.
  \par
  Методом редко пользуются потому что:
  \begin{enumerate}
  	\item Слишком много допущений;
  	\item в аналитическом виде решение частных производных вычислить трудно, т.к. матрицу Гессе трудно определить.
  	\item не учитываются ограничения.
  \end{enumerate}
  \section{Метод множителей лагранжа}
  Допущения:
  \begin{enumerate}
  	\item Функция гладкая и в каждой точке имеет 1-ю и 2-ю производные.
  	\item $K_\rho = K_{\rho0}; \; \rho = 1,2,\ldots,m$  		
  \end{enumerate}
  $$L(\bar{q}) = K(\bar{q}) + \sum_{\rho=1}^{m} \lambda_\rho[K_{\rho_0} - K_\rho(\bar{q})]$$
  $$\frac{\partial L}{\partial q_i} = \frac{\partial K}{\partial q_i} - \sum_{\rho=1}^{m} \lambda_\rho \frac{\partial K_\rho(\bar{q})}{\partial q_i} = 0; \; i=1,2,\ldots,n$$
  $$\frac{\partial L}{\partial \lambda_\rho} = [K_{\rho_0} - K_\rho] = 0$$
  \paragraph*{Пример}
  $max P$ \\
  m - масса \\
  v - объём \\
  c - экономия \\
  $$max P$$
  $$m(v,c) \leq m(v_0,c_0)$$
  \begin{center}
	либо $min(m,v,c)$ при $P \geq P_0$
  \end{center}
  \section{Поисковые методы}
  Рассмотрим условия, когда локальный экстремум является глобальным. Рассмотрим понятия выпуклой и вогнутой функции.\\
  $$z = \lambda (K(\bar{q}') + (1-\lambda)K(\bar{q}''))$$  
  \begin{tikzpicture}
    \draw[->] (-0.5,0)--(6,0) node[anchor=north] {$q$};
  	\draw[->] (0,-0.5)--(0,4) node[anchor=east] {$K$};
  	\draw[help lines] (1.5,0) node[below] {$q'$} -- ++(0,1.5) -- ++(3,1) -- ++(0,-2.5) node[below] {$q''$};
  	\draw (1.5,1.5) .. controls (2,2) and (3,3) .. (4.5,2.5);
  	\draw (1.5,1.5) .. controls (2,1) and (4,1.5) .. (4.5,2.5);
  	\draw (4,3) -- ++(0.5,0.75) node[anchor=south] {Выпуклая};
  	\draw (4,1.75) -- ++(1,-0.25) node[anchor=west] {Вогнутая};
  \end{tikzpicture}
  \\VERIFY  
  $$K[\lambda \bar{q}' + (1-\lambda+q'')] \leq Z = \lambda(K(\bar{q}' + (1-\lambda)K(\bar{q}'))$$
  Если знак неравенства $\leq$ - имеем дело с выпуклой функцией.\\
  Если неравенство строгое - строго выпуклая функция.\\
  Если знак $\geq$ - то функция вогнутая.\\
  
  Если на выпуклом множестве определена вогнутая функция, тогда локальный максимум является глобальным.
  
  \section{Метод Гаусса-Зейделя}
  Функция $K(q_1, q_2, \ldots, q_n);\;q_1, q_2, \ldots, q_n = const $
  \begin{enumerate}
    \item $max(min(K(q_1)))$ \\
      \begin{tikzpicture}
  	    \draw[->] (-0.5,0)--(4,0) node[anchor=north] {$q_1$};
  	    \draw[->] (0,-0.5)--(0,4) node[anchor=east] {$q$};
  	    \draw (2,2) -- +(-0.5,0) -- +(-0.5,0.5) -- +(0.5, 0.5) -- +(0.5,-0.5) -- +(-1,-0.5) -- +(-1,1) -- +(1,1) -- +(1,-1) -- +(-1,-1);
      \end{tikzpicture}
    \item $min(max(K(q_1)))$ \\
      \begin{tikzpicture}
        \draw[->] (-0.5,0)--(4,0) node[anchor=north] {$q$};
  	    \draw[->] (0,-0.5)--(0,4) node[anchor=east] {$K$};
  	    \draw (1,1) -- ++(1,0) -- ++(0,1) -- ++(0.5,0) -- ++(0,0.5) -- ++(0.25,0) -- ++(0,0.25);
      \end{tikzpicture}
  \end{enumerate}
  $$|q_1^m - q_1^r| \leq \epsilon_i - q_n$$
  $$|K^m - K^r| \leq \eta $$

  \section{Градиентный метод оптимизации (первого порядка)}
  Идея: ищут стационарную точку (безусловный экстремум) равных значений показателя.\\
  \begin{tikzpicture}
    \draw[->] (-0.5,0)--(6,0) node[anchor=north] {$q_1$};
  	\draw[->] (0,-0.5)--(0,5) node[anchor=east] {$q_2$};
  	\draw (3,2.9) circle[x radius=2mm, y radius=1mm, rotate=30];
  	\draw (2.5,2.5) .. controls +(1mm,6mm) and +(-1mm,4mm) .. (3.5,3)
  					.. controls +(0,-4mm) and +(0,-2mm) .. (2.5,2.5);
  	\draw (2,2) .. controls +(0,1) and +(-0.25,0.25) .. (3,3.5)
  				.. controls +(0.3,-0.2) and +(0.5,1) .. (3.5,2.5)
  				.. controls +(-0.2,-0.5) and +(0,-0.3) .. (2,2);
  	\draw (1.5,1.5) .. controls +(0.1,2.5) and +(0.3,2) .. ++(3	,2)
  					.. controls +(0,-1) .. ++(-0.5,-1.5)
  					.. controls +(-0.5,-0.5) and +(-0.1,-0.8) .. (1.5,1.5);
  	\coordinate (a) at (1,2);
  	\coordinate (b) at ($(a) +(49:4cm)$);
  	\draw[help lines] (a) -- (b);
  	\coordinate (c) at ($ (a)!0.50!(b) $);
  	\coordinate (d) at ($ (c)!1cm!-90:(b) $);
	\draw[help lines] (c) -- (d);
  	\draw (4.3,4.7) -- ++(1,0.25) node[anchor=west] {кривые равных значений};
	\draw (5.3,4.6) node[anchor=west] {показателя};
  \end{tikzpicture}\\
  Направление крутого восхождения:
  $$
    \nabla K(\bar{q}^{\,r}) =
    \begin{bmatrix}
      \frac{\partial k}{\partial q_1}\\
      \frac{\partial k}{\partial q_2}\\
      \vdots\\
      \frac{\partial k}{\partial q_n}\\
    \end{bmatrix}
  $$
  \par
  Направление наибольшего возрастания показателя и будет направлением градиента.
  \par
  Противоположное ему направление называется антиградиентом(отрицательным градиентом).
  \par
  С каждым шагом мы вычисляем новое направление градиента, с тем чтобы двигаться к экстремуму.
  $$\bar{q}^{\,2+1} = \bar{q}^{\,2} + \theta\nabla K(\bar{q}^{\,(2)})$$
  $$\bar{q}_i^{\,2+1} = \bar{q}^{\,2} + \theta\bigg(\frac{\partial k}{\partial q_i}\bigg)_k$$
  \par
  Иногда вектор градиент нормируется
  $$\nabla K^{\,r}_{\,n}(q^{\,r}) = \frac{\Delta K(q_2)}{\Big[\sum_{k=1}^n\big(\frac{\partial k}{\partial q_i}\big)\Big]^{\frac{1}{2}}}$$
  Выбор шага:
  \begin{enumerate}
    \item Метод с постоянным шагом
    \item С оптимальным шагом
  \end{enumerate}
  \section{Метод с оптимальным шагом}
  Есть $k$ в точке $r+1$
  $$K(\bar{q}^{r+1}) = K[\bar{q}^r + \Theta\nabla K(\bar{q}^r)]$$
  Ищем $\Theta$, которое соответствует $max(K(q^{r+1}))$ и $min$, если мы решаем задачу на минимум.\\
  Мы свели задачу многомерной оптимизации к задаче одномерной оптимизации. Условная функция должна быть гладкая и должны существовать по крайней мере две производные; и матрица Гессе должна быть знакоопределённая.\\
  Если мы попадём в седло, то из этой точки надо вылезать с помощью другого метода. Если функция гладкая и имеет 3 производных, то градиентный метод сходится при $r \rightarrow \infty$. Если функция квадратичная, то мы приходим к экстремуму за конечное число шагов.
  \paragraph*{Ограничение значений параметра}
  \begin{tikzpicture}
    \draw[->] (-0.5,0)--(7,0) node[anchor=north] {$q_1$};
  	\draw[->] (0,-0.5)--(0,5) node[anchor=east] {$q_2$};
    \draw (1,1) rectangle (6,4);  	
  	\coordinate (a) at (2,2.5);
  	\coordinate (b) at (5.5,5);
  	\coordinate (c) at ($ (a)!0.60!(b) $);
  	\draw                             (a) -- (b);
  	\draw [decorate,decoration={brace,raise=5pt}] (a) -- (b) node[pos=0.45,above=10pt] {$\Theta_{\text{опт}}$};
  	\draw [decorate,decoration={brace,mirror,raise=5pt}] (a) -- (c) node[pos=0.7,below=12pt] {$\Theta_{\text{доп}}$};
  	\draw[->] (5.25,4) -- +(0,-1);
  	\node[cross out, draw, inner sep=3pt] at (5.25,4) {};
  \end{tikzpicture}
  $$\Theta_r = min[\Theta_{\text{доп}} - \Theta_{\text{опт}}]$$
  $$\Theta_{\text{доп}} - \text{допустимое}$$
  $$\Theta_{\text{опт}} - \text{оптимальное}$$
  Отбрасываем переменную, которая вышла за границу и пытаемся решить задачу без неё. Вычисляем производную по $q_2$. Если она направлена вниз, то мы движемся внутрь области, если вверх - за пределы области; считаем эту точку оптимальной.\\
  Преимущества: 
  \begin{itemize}
    \item Классический эталонный метод.
  \end{itemize}
  Недостатки: 
  \begin{itemize}
    \item Сложность вычисления производных.
    \item Численные методы могут давать большие ошибки.
    \item Метод по своей сути не рассчитан на учёт ограничений.
  \end{itemize}
  \section{Градиентный метод (второго порядка)}
  $$K(q^{\,r+1}) = K(\bar{q}^{\,r}) + \sum_{i = 1}^n\bigg(\frac{\partial k}{\partial q_1}\bigg)_r\Delta q_ir + \frac{1}{2}\sum^{n}_{i,j=1}\bigg(\frac{\partial^{\,2}k}{\partial q_i\partial q_j}\bigg)_r\Delta q_ir\Delta q_jr$$
  $$\Delta q_ir = q_i^{\,r+1} - q_i^{\,r}$$
  \par
  В матричной форме
  \begin{equation}\label{eq:quad}
    \phi^{\,2} = \overbracket[0.5pt][7pt]{\nabla^{\,T}\underset{\mathclap{\begin{matrix}\uparrow\\\text{\scriptsize{вектор}}\\\text{\scriptsize{строка}}\end{matrix}}}{K}(\bar{q}^{\,r})\bar{\Delta}\underset{\mathclap{\begin{matrix}\uparrow\\\text{\scriptsize{вектор}}\\\text{\scriptsize{столбец}}\end{matrix}}}{q^{\,2}}}^{\mathclap{\text{при умножении получается число}}}+\frac{1}{2}(\Delta\bar{q}^{\,r})^{\,T}\mathcal{J}(\bar{q}^{\,r})_\Delta\bar{q}^{\,r}
  \end{equation}
  \par
  \ref{eq:quad} - квадратичная форма.
  \begin{equation}\label{eq:up}
    \frac{\partial \phi^{\,r}}{\partial \bar{\Delta}q^{\,r}}
  \end{equation}
  \par
  \ref{eq:up} - метод наискорейшего восхождения(спуска, если решаем задачу на $min$)
  \par
  В формуле \ref{eq:quad} возьмем производную по первому и второму слогаемому
  $$\nabla K(\bar{q}^{\,r}) + \mathcal{J}(\bar{q}^{\,r})\Delta\bar{q}^{\,r} = 0$$
  $$\mathcal{J}(q\,')\bar{\Delta} q^{\,r} = \nabla K(\bar{q}^{\,r})$$
  \par
  Умножаем на матрицу, обратную матрице $\mathcal{J}$.
  $$\underbrace{\mathcal{J}^{\,-1}(\bar{q}^{\,r})\mathcal{J}(\bar{q}^{\,r})}_{\mathclap{\text{E - единичная матрица}}}\bar{\Delta}q^{\,r} = \mathcal{J}^{\,-1}(q^{\,2})\nabla K(\bar{q}^{\,2})$$
  \par
  \begin{equation}\label{eq:cool}
    \bar{\Delta}q^{\,r} = \mathcal{J}^{\,-1}(q^{\,r})\nabla K(\bar{q}^{\,r})
  \end{equation}
  \par
  \ref{eq:cool} - направление самого крутого восхождения или наискорейшего спуска.
  \begin{equation}
    \bar{q}^{\,r+1} = \bar{q}^{\,r} - \theta\mathcal{J}^{\,-1}(\bar{q}^{\,r})\nabla K(\bar{q}^{\,r})
  \end{equation}
  \par
  Окончательное выражение для градиентного метода второго порядка(метода Ньютона)
  \par
  Если функция $K$ - квадратичная, то оптимальная точка достигается за один шаг и точка берется равной 1.
  $$
    \mathcal{J} =
    \begin{bmatrix}
      a_{11} & a_{12}\\
      a_{21} & a_{22}
    \end{bmatrix}
  $$
  $$
    \tilde{\mathcal{J}} =
    \begin{bmatrix}
      a_{11} & -a_{12}\\
      -a_{21} & a_{22}
    \end{bmatrix}
  $$
  $$
    \mathcal{J} = \frac{1}{\Delta}\mathcal{J}
  $$
  \paragraph*{Задача}
  $$K = (q_1 - 5)^2 + (q_2 - 8)^2 + (q_1 - q_2)^2$$
  Найти минимум функции.\\
  Начальные условия:\\
  $q^0 = 
  \begin{bmatrix}
    0 \\ 
    0    
  \end{bmatrix}$
  $$\frac{\partial K}{\partial q_1} = 2(q_1 - 5) + 2(q_1 - q_2) = -10$$
  $$\frac{\partial K}{\partial q_2} = 2(q_2 - 8) - 2(q_1 - q_2) = -16$$
  $$\frac{\partial^2K}{\partial q_1^2} = 2 + 2 = 4$$
  $$\frac{\partial^2K}{\partial q_2^2} = 2 + 2 = 4$$
  $$\frac{\partial^2K}{\partial q_1 \partial q_2} = -2$$  
  $$\mathcal{J} = 
  \begin{bmatrix}
    4 & -2 \\
    -2 & 4
  \end{bmatrix} - \text{Матрица Гессе}$$
  $$\mathcal{\tilde{J}} = 
  \begin{bmatrix}
    4 & 2 \\
    2 & 4
  \end{bmatrix}  
  $$
  $$\Delta = 16 - 4 = 12$$
  $$\mathcal{J}^{-1}(\bar{q}) = \frac{1}{12}\begin{bmatrix}
    4 & 2 \\
    2 & 4
  \end{bmatrix}$$
  $$\nabla K = \begin{bmatrix}
    -10 \\
    -16
  \end{bmatrix}$$
  Всё это справедливо только в нулевой точке.
  $$\bar{q}_0 = \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} - \begin{bmatrix}
    \frac{1}{3} & \frac{1}{6} \\[0.3em]
    \frac{1}{6} & \frac{1}{3}
  \end{bmatrix} \ast \begin{bmatrix}
    -10 \\
    -16
  \end{bmatrix} = \begin{bmatrix}
    0 \\
    0
  \end{bmatrix} + \begin{bmatrix}
    \frac{1}{3}\cdot 10 + \frac{1}{6}\cdot 16 \\
    \frac{1}{6}\cdot 10 + \frac{1}{3}\cdot 16
  \end{bmatrix} = \begin{bmatrix}
    6 \\
    7
  \end{bmatrix}$$
  \textit{Перемножение матриц: первая строка на первый столбец; вторая строка на первый столбец.}\\
  Преимущество: высокая скорость сходимости.
  Недостаток: сложность вычисления матрицы Гессе. 
\end{document}
